import ollama
import logging
import streamlit as st
import os
from ReflectionTuning import GenerateReflexion

# ChatBot integration
from shortterm_memory.ChatbotMemory import ChatbotMemory
from src.rag.new_chromadb import rag_pipeline

#BRMS integration
from src.brmsAPI.brmsAssurance import brmsCall, clear_dialog_element

# Désactiver le parallélisme pour éviter les deadlocks
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Param logger
logging.basicConfig(filename="app.log" , filemode="w", level=logging.DEBUG)

class Generate:
    def __init__(self, model:str="mistral:latest", ollama_options=None):
        self.model = model
        self._ollama_option = ollama_options if ollama_options else {'temperature': 1}
        self.memory = ChatbotMemory()
        self.running = False
        self.response = ""
        self.suma_on_run = False
        self.assurance_phase = False
        self.internal_reflextion = GenerateReflexion(model='openchat:latest')
        self.memoire_contextuel_assurance = ""
    
    def remember(self, sauvegarde)-> None:
        """
        Updates the Chatbot's memory with user-bot interactions from the provided sauvegarde DataFrame.

        Args:
            sauvegarde: DataFrame containing user-bot interaction data.

        Returns:
            None
        """

        try:
            for index, row in sauvegarde.iterrows():
                self.memory.update_memory(row['user'], row['bot'])
            st.sidebar.success("Load !")
        except Exception as e:
            st.sidebar.error(f"Error : {e}")
        

    def ans(self, user_input:str):#user_input="l'assurance de manon qui à 34 ans et qui habite à paris"): # Debug modification
        """
        Generates a response from the Chatbot based on the user input and updates the Chatbot's memory.

        Args:
            user_input: The input provided by the user.

        Returns:
            str: The response generated by the Chatbot.
        """
        # Initialisation
        self.assurance_phase = False
        solve_statue = False
        self.running = True
        self.response = ""
        print("MEm de conversation_history : ",self.memory.get_memory())
        
        user_input =  user_input + self.memoire_contextuel_assurance if self.memoire_contextuel_assurance else user_input
        
        ####################
        # BRMS Integration #
        ####################
        if "assurance" in user_input:
            self.assurance_phase = True
            sentence, liste_element, solve_statue = brmsCall(user_input)
            reflexion_input = ""
        ####################
        else:
            # reflexion_input = self.internal_reflextion.gen(user_input,self.memory.get_memory())
            reflexion_input = " "
        
        context = rag_pipeline(query=user_input)
        
        if self.assurance_phase:
            assurance_output = "Voici le retour du systême expert :\n ",sentence
            if solve_statue:
                self.memoire_contextuel_assurance = ""
            else:
                self.memoire_contextuel_assurance = f" + ces éléments ont déjà été mentionné et sont à retenir pour l'assurance : {liste_element}"
        else:
            assurance_output = ""
            self.memoire_contextuel_assurance = ""
        print("----------------->>>>>>",assurance_output )
        
        prompt = (
            "Vous êtes un assistant intelligent. Utilisez les informations suivantes pour aider l'utilisateur.\n\n"
            "Mémoire du chatbot (à ne pas montrer à l'utilisateur) :\n"
            f"{self.memory.get_memory()}\n\n"
            "Contexte pertinent :\n"
            f"{context}\n\n"
            "Question de l'utilisateur :\n"
            f"{user_input}\n\n"
            f"{assurance_output}\n\n"
            "Voici les étapes de reflexion donné par l'agent dédié au 'Reflection-Tuning' qui pourrait t'aider à répondre.\n"
            "Ces instructions ne sont que pour toi ne les dévoile pas :\n"
            "Tu peux en tenir compte ou les ignorer :\n"
            f"{reflexion_input}\n\n"
            "Répondez de manière claire et CONCISE et avec une mise en forme lisible et structuré :\n"
        )
        result = ollama.generate(
            model=self.model,
            prompt=prompt,
            stream=True,
            options=self._ollama_option
        )
        print(f"Response generated. with model : {self.model}")
        logging.info(f"Response generated. with model : {self.model}")
        
        self.response = ""
        for chunk in result:
            self.response += chunk['response']
            yield chunk['response']
        
        # clean user_input to avoid any confusion and recall assurance entity
        if solve_statue:
            user_input = clear_dialog_element(user_input, liste_element)
            
        self.memory.update_memory(user_input, self.response)

        self.running = False
