import ollama
import logging
import re

from src.memory import ChatbotMemory, memory_counter, compressed_memory
from src.rag.new_chromadb import rag_pipeline
import src.brmsAPI.api as ap
import src.brmsAPI.payload_construction as pc

import streamlit as st
import os

# Désactiver le parallélisme pour éviter les deadlocks
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# Param logger
logging.basicConfig(filename="app.log" , filemode="w", level=logging.DEBUG)

class Generate:
    def __init__(self, model:str="openchat:latest", ollama_options=None):
        self.model = model
        self._ollama_option = ollama_options if ollama_options else {'temperature': 1}
        self.memory = ChatbotMemory()
        self.running = False
        self.response = ""
        self.suma_on_run = False
        self.assurance_phase = False
    
    def remember(self, sauvegarde)-> None:
        """
        Updates the Chatbot's memory with user-bot interactions from the provided sauvegarde DataFrame.

        Args:
            sauvegarde: DataFrame containing user-bot interaction data.

        Returns:
            None
        """

        try:
            for index, row in sauvegarde.iterrows():
                self.memory.update_memory(row['user'], row['bot'])
            st.sidebar.success("Load !")
        except Exception as e:
            st.sidebar.error(f"Error : {e}")
        

    def ans(self, user_input="l'assurance de manon qui à 34 ans et qui habite à paris"): # Debug modification
        """
        Generates a response from the Chatbot based on the user input and updates the Chatbot's memory.

        Args:
            user_input: The input provided by the user.

        Returns:
            str: The response generated by the Chatbot.
        """
        # Initialisation
        self.assurance_phase = False
        self.running = True
        self.response = ""
        print("MEm de conversation_history : ",self.memory.get_memory())
        
        
        ####################
        # BRMS Integration #
        ####################
        if "assurance" in user_input:
            
            self.assurance_phase = True

            # payload = {"__DecisionID__": "exampleID","contract": {"id": 12345,"clients": [{"nom": nom,"prenom": prenom,"age": age,"adresse": adresse}],"montant": 0}}
            request = (
                "Tu es un expêrt en data capture, ton role est d'extraire uniquement les données requises.\n\n"
                "Extrait les informations au format liste suivant :\n Nom ; Prenom ; Age ; Adresse.\n\n."
                "Si il y a des informations manquantes laisse les zonnes vides.\n\n"
                "Si il y a des données manquantes n'écris rien, laise vide.\n\n"
                "Ne répond rien d'autre que la liste. Nom ; Prenom ; Age ; Adresse] .\n\n"
                "Voici quelques exemples pour te montrer: \n\n"
                "Exemple numéro 1; utilisateur:'Je veux une information sur une assurance, il s'agit de madame Durant Jenny' reponse:Durant ; Jenny.\n\n"
                "Exemple numéro 2; utilisateur:'Nous voulons les données d'assurance de monsieur Alexandre Gigof agé de 56 ans et résident au 6 rue labradore, Paris' reponse: Gigof ; Alexandre ; 56 ; 6 rue labradore, Paris .\n\n"
                "Exemple numéro 3; utilisateur:'' reponse: .\n\n"
                "Exemple numéro 4; utilisateur:'j'aurais une question sur l assurance' reponse: .\n\n"
                "Voici la phrase cible: " f"{user_input}"
                )
            
            elements = ollama.generate(
                # model=self.model,
                model="mistral:latest",
                prompt=request,
                stream=False,
                options=self._ollama_option
            )
            print("------------------------------------------------>>>>>>------------------------------------------------>>>>>>",elements["response"])
            
            elements2 = re.sub(r'[^a-zA-Z0-9;]', '', elements["response"])
            elements3 = elements2.split(';')
            print(elements3)
            print(elements3[0])
            payload = pc.payload_construction(nom=elements3[0], prenom=elements3[1], age=elements3[2], adresse=elements3[3])
            
            api = ap.ApiCall(url="http://10.21.8.3:9090/DecisionService/rest/v1/assurance_deploy/OD_assurance/", payload=payload, headers={'Content-Type': 'application/json'})
            test_completion = api.test_arguments()
            print("------------------------------------------------>>>>>>", test_completion)
            if test_completion is not None:
                sentence = "Tu dois indiquer à ton interlocuteur que tu ne peux pas répondre pour la raison suivante : ", test_completion
            else:
                sentence = "D'après les informations que tu as renseignée la réponse est : ", api.call_api()
        ####################
        
        context = rag_pipeline(query=user_input)
        
        if self.assurance_phase:
            assurance_output = "Voici le retour du systême expert :\n ",sentence
        else:
            assurance_output = ""
        print("----------------->>>>>>",assurance_output )
        prompt = (
            "Vous êtes un assistant intelligent. Utilisez les informations suivantes pour aider l'utilisateur.\n\n"
            "Mémoire du chatbot (à ne pas montrer à l'utilisateur) :\n"
            f"{self.memory.get_memory()}\n\n"
            "Contexte pertinent :\n"
            f"{context}\n\n"
            "Question de l'utilisateur :\n"
            f"{user_input}\n\n"
            f"{assurance_output}\n\n"
            "Répondez de manière claire et concise :\n"
        )
        result = ollama.generate(
            model=self.model,
            prompt=prompt,
            # stream=False,
            stream=True,
            options=self._ollama_option
        )
        print(f"Response generated. with model : {self.model}")
        logging.info(f"Response generated. with model : {self.model}")
        
        self.response = ""
        for chunk in result:
            self.response += chunk['response']
            yield chunk['response']
        
        self.memory.update_memory(user_input, self.response)

        # TODO: effectuer cette tache en async pour eviter qu'elle ne ralentisse tout le processus 
        if memory_counter(self.memory.get_memory()) > 500 and self.suma_on_run == False:
            self.suma_on_run = True
            print("Conversation_history : ",self.memory.get_memory())
            self.memory = ChatbotMemory(compressed_memory(self.memory.get_memory()))
            print("Compressed conversation_history : ",self.memory.get_memory())
            self.suma_on_run = False
            logging.info("Memory compressed.")

        self.running = False
